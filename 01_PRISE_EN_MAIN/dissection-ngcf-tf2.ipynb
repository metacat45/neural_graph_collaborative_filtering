{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.8"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Prise en main du code de l'article NGCF\n","metadata":{}},{"cell_type":"markdown","source":"La prise en main du code du site compagnon n'est pas aisé car le code enchaine de multiples fonctions et méthodes.\nL'objectif de ce notebook est donc de disséquer le code du site compagnon afin d'en comprendre les rouages et de revenir aux fondements théoriques de l'article.\n( pour une lecture avec un plan, utiliser jupyter lab)\n\nCela permet également de voir comment réécrire le code en tensorflow 2.x.\nDans les points TF non compatibles ont a le placeholder, l'initialisation de Glorot, random_uniform, sparse_retain, tf.div, tf.sparse_dense_matmul\n\n\n\n","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\n\nfrom utility.load_data import *\n","metadata":{},"execution_count":99,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A des fins exploratoires les attributs de l'objet NGCF sont passés en paramètres.\n\nPour faciliter la lecture on explore le code qu'avec une profondeur de 2 couches.","metadata":{}},{"cell_type":"code","source":"#donnees ml100\nn_users = 943\nn_items = 1682\nemb_dim = 40 #hyperparamètre\nweight_size = [64,64]  #('--layer_size', nargs='?', default='[64]', help='Output sizes of every layer')\nn_layers = len(weight_size)  #[64] -> 1 couche.  [64, 64] -> 2 couches\nbatch_size = 50  # on prend une valeur différente de 64 pour bien distinguer les paramètres\ndecay = 0.1\nmess_dropout =  [0.1,0.1]\n#dans un batch on va avoir des items qui seront à la fois positifs et négatifs pour les 2 listes\n\nn_fold = 100\nkeep_prob = 0.7\nnode_dropout = [0.7]","metadata":{},"execution_count":145,"outputs":[]},{"cell_type":"code","source":"from os import getcwd\ngetcwd()","metadata":{},"execution_count":146,"outputs":[{"execution_count":146,"output_type":"execute_result","data":{"text/plain":"'/home/jovyan/MonDossier/neural_graph_collaborative_filtering/prise_en_main'"},"metadata":{}}]},{"cell_type":"markdown","source":"## Création des poids","metadata":{}},{"cell_type":"code","source":"\n# A verifier mais les poids MLP ne sont que pour le modèle GMC\n\ndef _init_weights(emb_dim, weight_size, n_layers):\n        all_weights = dict()\n\n        initializer = tf.keras.initializers.GlorotUniform()\n        all_weights['user_embedding'] = tf.Variable(initializer([n_users, emb_dim]), name='user_embedding')\n        all_weights['item_embedding'] = tf.Variable(initializer([n_items, emb_dim]), name='item_embedding')\n        \n        weight_size_list = [emb_dim] + weight_size\n\n        for k in range(n_layers):\n            all_weights['W_gc_%d' %k] = tf.Variable(\n                initializer([weight_size_list[k], weight_size_list[k+1]]), name='W_gc_%d' % k)\n            all_weights['b_gc_%d' %k] = tf.Variable(\n                initializer([1, weight_size_list[k+1]]), name='b_gc_%d' % k)\n\n            all_weights['W_bi_%d' % k] = tf.Variable(\n                initializer([weight_size_list[k], weight_size_list[k + 1]]), name='W_bi_%d' % k)\n            all_weights['b_bi_%d' % k] = tf.Variable(\n                initializer([1, weight_size_list[k + 1]]), name='b_bi_%d' % k)\n\n            #all_weights['W_mlp_%d' % k] = tf.Variable(\n            #    initializer([weight_size_list[k], weight_size_list[k+1]]), name='W_mlp_%d' % k)\n            #all_weights['b_mlp_%d' % k] = tf.Variable(\n            #    initializer([1, weight_size_list[k+1]]), name='b_mlp_%d' % k)\n\n        return all_weights","metadata":{},"execution_count":171,"outputs":[]},{"cell_type":"code","source":"weights = _init_weights( emb_dim = emb_dim, weight_size = weight_size, n_layers = n_layers)","metadata":{},"execution_count":172,"outputs":[]},{"cell_type":"code","source":"for cle, valeur in weights.items():\n    print(cle, \"shape : \",valeur.shape)","metadata":{},"execution_count":173,"outputs":[{"name":"stdout","output_type":"stream","text":"user_embedding shape :  (943, 40)\nitem_embedding shape :  (1682, 40)\nW_gc_0 shape :  (40, 64)\nb_gc_0 shape :  (1, 64)\nW_bi_0 shape :  (40, 64)\nb_bi_0 shape :  (1, 64)\nW_gc_1 shape :  (64, 64)\nb_gc_1 shape :  (1, 64)\nW_bi_1 shape :  (64, 64)\nb_bi_1 shape :  (1, 64)\n"}]},{"cell_type":"markdown","source":"## Accès aux données","metadata":{}},{"cell_type":"code","source":"# users, pos_items, neg_items = data_generator.sample()\n\n# un neg_item c'est un item du train qui n'est pas dans le batch\n\ndata_generator = Data(path='../Data/ml-100k', batch_size=batch_size)\n\n","metadata":{},"execution_count":174,"outputs":[{"name":"stdout","output_type":"stream","text":"n_users=943, n_items=1682\nn_interactions=100000\nn_train=90404, n_test=9596, sparsity=0.06305\n"}]},{"cell_type":"markdown","source":"Pour générer un batch on exécute la méthode sample :","metadata":{}},{"cell_type":"code","source":"users, pos_items, neg_items = data_generator.sample()","metadata":{},"execution_count":175,"outputs":[]},{"cell_type":"code","source":"#pos_items, users","metadata":{},"execution_count":176,"outputs":[]},{"cell_type":"markdown","source":"On remarque que,marginalement, pour un batch donné **des items sont à la fois perçus comme positifs et négatifs** (faire tourner plusieurs fois la cellule data_generator.sample() si besoin)","metadata":{}},{"cell_type":"code","source":"len(list(set(pos_items) & set(neg_items)))","metadata":{},"execution_count":177,"outputs":[{"execution_count":177,"output_type":"execute_result","data":{"text/plain":"2"},"metadata":{}}]},{"cell_type":"markdown","source":"## Aperçu de la loss","metadata":{}},{"cell_type":"markdown","source":"**ATTENTION** ne sera pertinent qu'après avoir transformé les intrants en embedding","metadata":{}},{"cell_type":"markdown","source":"On constate qu'avec un argument négatif très fort la log sigmoid retourne - $\\infty$  : ","metadata":{}},{"cell_type":"code","source":"def create_bpr_loss(decay, batch_size, users, pos_items, neg_items):\n        pos_scores = tf.reduce_sum(tf.multiply(users, pos_items))\n        neg_scores = tf.reduce_sum(tf.multiply(users, neg_items))\n\n        regularizer = tf.nn.l2_loss(users) + tf.nn.l2_loss(pos_items) + tf.nn.l2_loss(neg_items)\n        regularizer = regularizer/batch_size\n        \n        # In the first version, we implement the bpr loss via the following codes:\n        # We report the performance in our paper using this implementation.\n        maxi = tf.math.log(tf.nn.sigmoid(pos_scores - neg_scores))\n        print(\"Maxi : \",maxi)\n        mf_loss = tf.negative(tf.reduce_mean(maxi))\n        print(\"MF loss V1 :\",mf_loss)\n        ## In the second version, we implement the bpr loss via the following codes to avoid 'NAN' loss during training:\n        ## However, it will change the training performance and training performance.\n        ## Please retrain the model and do a grid search for the best experimental setting.\n        mf_loss2 = tf.reduce_sum(tf.nn.softplus(-(pos_scores - neg_scores)))\n        print(\"MF loss V2 :\",mf_loss2)\n\n        emb_loss = decay * regularizer\n\n        reg_loss = tf.constant(0.0, tf.float32, [1])\n\n        return pos_scores, neg_scores,  mf_loss, emb_loss, reg_loss","metadata":{},"execution_count":179,"outputs":[]},{"cell_type":"code","source":"users2     = tf.constant(users, dtype='float32')\npos_items2 = tf.constant(pos_items, dtype='float32')\nneg_items2 = tf.constant(neg_items, dtype='float32')\n\ntf.multiply(users2, pos_items2)\npos_scores = tf.reduce_sum(tf.multiply(users2, pos_items2))\nneg_scores = tf.reduce_sum(tf.multiply(users2, neg_items2))\nprint(pos_scores - neg_scores)\n\n","metadata":{},"execution_count":180,"outputs":[{"name":"stdout","output_type":"stream","text":"tf.Tensor(-6451864.0, shape=(), dtype=float32)\n"}]},{"cell_type":"code","source":"create_bpr_loss(decay=decay, batch_size=batch_size, users=users2, pos_items=pos_items2, neg_items=neg_items2)","metadata":{},"execution_count":181,"outputs":[{"name":"stdout","output_type":"stream","text":"Maxi :  tf.Tensor(-inf, shape=(), dtype=float32)\nMF loss V1 : tf.Tensor(inf, shape=(), dtype=float32)\nMF loss V2 : tf.Tensor(6451864.0, shape=(), dtype=float32)\n"},{"execution_count":181,"output_type":"execute_result","data":{"text/plain":"(<tf.Tensor: shape=(), dtype=float32, numpy=9452200.0>,\n <tf.Tensor: shape=(), dtype=float32, numpy=15904064.0>,\n <tf.Tensor: shape=(), dtype=float32, numpy=inf>,\n <tf.Tensor: shape=(), dtype=float32, numpy=68706.85>,\n <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>)"},"metadata":{}}]},{"cell_type":"markdown","source":"L'ordre de grandeur de la loss v2 peut devenir trop important et je garde en tête d'appliquer éventuellement un **clipping** car on peut avoir des risques d'explosion de gradient.","metadata":{}},{"cell_type":"markdown","source":"## Matrice d'adjacence","metadata":{}},{"cell_type":"markdown","source":"L'objet Data dans le module utiity/load_data contient l'outillage pour générer des matrices d'adjacence et les sauvegardée sur disque. Si les matrices ne sont pas déjà existentes elles sont crées alors dans le dossier Data du jeu de données en cours.","metadata":{}},{"cell_type":"code","source":"plain_adj, norm_adj, mean_adj = data_generator.get_adj_mat()","metadata":{},"execution_count":193,"outputs":[{"name":"stdout","output_type":"stream","text":"already load adj matrix (2625, 2625) 0.0281069278717041\n"}]},{"cell_type":"markdown","source":"3 matrices sont ainsi crées, on n'en utilise qu'une seule selon l'option précisée adj_type. J'ai une préférence pour une matrice normalisée.  ","metadata":{}},{"cell_type":"code","source":"norm_adj","metadata":{},"execution_count":194,"outputs":[{"execution_count":194,"output_type":"execute_result","data":{"text/plain":"<2625x2625 sparse matrix of type '<class 'numpy.float64'>'\n\twith 183433 stored elements in Compressed Sparse Row format>"},"metadata":{}}]},{"cell_type":"markdown","source":"On regarde les valeurs de quelques éléments :","metadata":{}},{"cell_type":"code","source":"norm_adj[0,0],  norm_adj[25,30] , norm_adj[100,100]","metadata":{},"execution_count":195,"outputs":[{"execution_count":195,"output_type":"execute_result","data":{"text/plain":"(0.0040650406504065045, 0.0, 0.016129032258064516)"},"metadata":{}}]},{"cell_type":"markdown","source":"Pour la suite on a besoin de l'attribut n_nonzero_elems :\n","metadata":{}},{"cell_type":"code","source":"n_nonzero_elems = norm_adj.count_nonzero()\nn_nonzero_elems","metadata":{},"execution_count":196,"outputs":[{"execution_count":196,"output_type":"execute_result","data":{"text/plain":"183433"},"metadata":{}}]},{"cell_type":"markdown","source":"# Création des embeddings","metadata":{}},{"cell_type":"markdown","source":"Le code gère la problématique de saturation RAM en splittant la matrice d'adjacence dans une liste de sous-matrice et en prenant en compte possiblement l'utilisation de drop out.\n\nOn a besoin de pas mal d'outillage pour créer les embeddings :","metadata":{}},{"cell_type":"code","source":"def _create_ngcf_embed(node_dropout_flag , norm_adj, weights, mess_dropout, n_users, n_items, n_layers, n_fold , node_dropout):\n# Generate a set of adjacency sub-matrix.\n    if node_dropout_flag:\n        # node dropout.\n        A_fold_hat = _split_A_hat_node_dropout(norm_adj)\n    else:\n        A_fold_hat = _split_A_hat(norm_adj)\n\n    ego_embeddings = tf.concat([weights['user_embedding'], weights['item_embedding']], axis=0)\n\n    all_embeddings = [ego_embeddings]\n\n    for k in range(0, n_layers):\n\n        temp_embed = []\n        for f in range(n_fold):\n            temp_embed.append(tf.sparse.sparse_dense_matmul(A_fold_hat[f], ego_embeddings))\n\n        # sum messages of neighbors.\n        side_embeddings = tf.concat(temp_embed, 0)\n        print('shape side_embeddings : ',side_embeddings.shape)\n        \n        # transformed sum messages of neighbors.\n        sum_embeddings = tf.nn.leaky_relu( tf.matmul(side_embeddings, weights['W_gc_%d' % k]) + weights['b_gc_%d' % k] )\n        print('shape sum_embeddings : ', sum_embeddings.shape)\n\n        # bi messages of neighbors.\n        bi_embeddings = tf.multiply(ego_embeddings, side_embeddings)\n        print('shape bi_embeddings : ', bi_embeddings.shape)\n        # transformed bi messages of neighbors.\n        bi_embeddings = tf.nn.leaky_relu( tf.matmul(bi_embeddings, weights['W_bi_%d' % k]) + weights['b_bi_%d' % k] )\n        print('shape bi_embeddings transformed : ', bi_embeddings.shape)\n        \n        # non-linear activation.\n        ego_embeddings = sum_embeddings + bi_embeddings\n        print('shape ego embeddings : ', ego_embeddings.shape)\n\n        # message dropout.\n        ego_embeddings = tf.nn.dropout(ego_embeddings, 1 - mess_dropout[k])\n\n        # normalize the distribution of embeddings.\n        norm_embeddings = tf.nn.l2_normalize(ego_embeddings, axis=1)\n\n        all_embeddings += [norm_embeddings]\n        print('****** fin boucle for *****')\n\n    all_embeddings = tf.concat(all_embeddings, 1)\n    print('shape all_embeddings : ', all_embeddings.shape)\n    u_g_embeddings, i_g_embeddings = tf.split(all_embeddings, [n_users, n_items], 0)\n    print('shape u_g_embeddings et i_g : ', u_g_embeddings.shape, i_g_embeddings.shape)\n    return u_g_embeddings, i_g_embeddings\n\n\n\ndef _convert_sp_mat_to_sp_tensor(X):\n    coo = X.tocoo().astype(np.float32)\n    indices = np.mat([coo.row, coo.col]).transpose()\n    return tf.SparseTensor(indices, coo.data, coo.shape)\n\ndef _dropout_sparse(X, keep_prob, n_nonzero_elems):\n    \"\"\"\n    Dropout for sparse tensors.\n    \"\"\"\n    noise_shape = [n_nonzero_elems]\n    random_tensor = keep_prob\n    random_tensor += tf.random.uniform(noise_shape)\n    dropout_mask = tf.cast(tf.floor(random_tensor), dtype=tf.bool)\n    pre_out = tf.sparse.retain(X, dropout_mask)\n\n    return pre_out * tf.math.divide(1., keep_prob)\n\n\n\ndef _split_A_hat(X):\n    A_fold_hat = []\n\n    fold_len = (n_users + n_items) // n_fold\n    for i_fold in range(n_fold):\n        start = i_fold * fold_len\n        if i_fold == n_fold -1:\n            end = n_users + n_items\n        else:\n            end = (i_fold + 1) * fold_len\n\n        A_fold_hat.append(_convert_sp_mat_to_sp_tensor(X[start:end]))\n    return A_fold_hat\n\ndef _split_A_hat_node_dropout( X):\n    A_fold_hat = []\n\n    fold_len = (n_users + n_items) // n_fold\n    for i_fold in range(n_fold):\n        start = i_fold * fold_len\n        if i_fold == n_fold -1:\n            end = n_users + n_items\n        else:\n            end = (i_fold + 1) * fold_len\n\n        # A_fold_hat.append(self._convert_sp_mat_to_sp_tensor(X[start:end]))\n        temp = _convert_sp_mat_to_sp_tensor(X[start:end])\n        n_nonzero_temp = X[start:end].count_nonzero()\n        A_fold_hat.append(_dropout_sparse(temp, 1 - node_dropout[0], n_nonzero_temp))\n\n    return A_fold_hat\n\n","metadata":{},"execution_count":197,"outputs":[]},{"cell_type":"code","source":"ua_embeddings, ia_embeddings = _create_ngcf_embed(node_dropout_flag=1 , norm_adj = norm_adj , weights = weights , node_dropout = node_dropout, mess_dropout = mess_dropout, n_users = n_users , n_items = n_items,n_layers = n_layers, n_fold = 100)","metadata":{},"execution_count":198,"outputs":[{"name":"stdout","output_type":"stream","text":"shape side_embeddings :  (2625, 40)\nshape sum_embeddings :  (2625, 64)\nshape bi_embeddings :  (2625, 40)\nshape bi_embeddings transformed :  (2625, 64)\nshape ego embeddings :  (2625, 64)\n****** fin boucle for *****\nshape side_embeddings :  (2625, 64)\nshape sum_embeddings :  (2625, 64)\nshape bi_embeddings :  (2625, 64)\nshape bi_embeddings transformed :  (2625, 64)\nshape ego embeddings :  (2625, 64)\n****** fin boucle for *****\nshape all_embeddings :  (2625, 168)\nshape u_g_embeddings et i_g :  (943, 168) (1682, 168)\n"}]},{"cell_type":"markdown","source":"**Commentaires**\n\nOn note le passage entre ego_embeddings et all_ebeddings :\n\n\n    L'opérateur += va ajouter le nb de colonnes en concatenant chaque embeddings d'une couche.\n    \n    Deux layers à 64 units donne une sortie à 128, trois layers donne 64*3\n\nLes premières lignes de all_ebeddings sont pour les users et les dernièrs pour les items.\n\n**Dimensions de sortie** \n\nPour emb_dim = 40 et 2 layers de 64 on obtient **40 + 64 + 64 = 168 colonnes**\n\nOn peut maintenant obtenir les coordonnées des users, des items positifs et négatif pour notre batch en cours dans notre espace :","metadata":{}},{"cell_type":"code","source":"#  Establish the final representations for user-item pairs in batch.\nu_g_embeddings = tf.nn.embedding_lookup(ua_embeddings    , users)\npos_i_g_embeddings = tf.nn.embedding_lookup(ia_embeddings, pos_items)\nneg_i_g_embeddings = tf.nn.embedding_lookup(ia_embeddings, neg_items)","metadata":{},"execution_count":199,"outputs":[]},{"cell_type":"code","source":"u_g_embeddings.shape, pos_i_g_embeddings.shape, neg_i_g_embeddings.shape","metadata":{},"execution_count":200,"outputs":[{"execution_count":200,"output_type":"execute_result","data":{"text/plain":"(TensorShape([50, 168]), TensorShape([50, 168]), TensorShape([50, 168]))"},"metadata":{}}]},{"cell_type":"markdown","source":"On peut mainenant réaliser une inférence (ce sera utile en testing):","metadata":{}},{"cell_type":"code","source":"batch_ratings = tf.matmul(u_g_embeddings, pos_i_g_embeddings, transpose_a=False, transpose_b=True)\nbatch_ratings.shape","metadata":{},"execution_count":201,"outputs":[{"execution_count":201,"output_type":"execute_result","data":{"text/plain":"TensorShape([50, 50])"},"metadata":{}}]},{"cell_type":"code","source":"_, _, mf_loss, emb_loss, reg_loss = create_bpr_loss(decay=decay, batch_size=batch_size, users=u_g_embeddings, pos_items=pos_i_g_embeddings, neg_items=neg_i_g_embeddings )\nloss = mf_loss + emb_loss + reg_loss\nprint(mf_loss,emb_loss,reg_loss, loss)","metadata":{},"execution_count":202,"outputs":[{"name":"stdout","output_type":"stream","text":"Maxi :  tf.Tensor(-0.49957103, shape=(), dtype=float32)\nMF loss V1 : tf.Tensor(0.49957103, shape=(), dtype=float32)\nMF loss V2 : tf.Tensor(0.49957103, shape=(), dtype=float32)\ntf.Tensor(0.49957103, shape=(), dtype=float32) tf.Tensor(0.3084888, shape=(), dtype=float32) tf.Tensor([0.], shape=(1,), dtype=float32) tf.Tensor([0.8080598], shape=(1,), dtype=float32)\n"}]}]}