{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.8"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Prise en main du code de l'article NGCF\n","metadata":{}},{"cell_type":"markdown","source":"La prise en main du code du site compagnon n'est pas aisé car le code enchaine de multiples fonctions et méthodes.\nL'objectif de ce notebook est donc de disséquer le code du site compagnon afin d'en comprendre les rouages et de revenir aux fondements théoriques de l'article.\n( pour une lecture avec un plan, utiliser jupyter lab)\n\nCela permet également de voir comment réécrire le code en tensorflow 2.x.\nDans les points TF non compatibles ont a le placeholder, l'initialisation de Glorot, random_uniform, sparse_retain, tf.div, tf.sparse_dense_matmul\n\n\n\n","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\n\nfrom utility.load_data import *\n","metadata":{"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"A des fins exploratoires les attributs de l'objet NGCF sont passés en paramètres :","metadata":{}},{"cell_type":"code","source":"#donnees ml100\nn_users = 943\nn_items = 1682\nemb_dim = 40 #hyperparamètre\nweight_size = [64,64]  #('--layer_size', nargs='?', default='[64]', help='Output sizes of every layer')\nn_layers = len(weight_size)  #[64] -> 1 couche.  [64, 64] -> 2 couches\nbatch_size = 64\ndecay = 0.1\nmess_dropout =  [0.1,0.1,0.1]\n#dans un batch on va avoir des items qui seront à la fois positifs et négatifs pour les 2 listes\n\nn_fold = 100\nkeep_prob = 0.7\nnode_dropout = [0.7]","metadata":{"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"from os import getcwd\ngetcwd()","metadata":{"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"'/home/jovyan/MonDossier/neural_graph_collaborative_filtering/prise_en_main'"},"metadata":{}}]},{"cell_type":"markdown","source":"## Création des poids","metadata":{}},{"cell_type":"code","source":" def _init_weights(emb_dim, weight_size, n_layers):\n        all_weights = dict()\n\n        initializer = tf.keras.initializers.GlorotUniform()\n        all_weights['user_embedding'] = tf.Variable(initializer([n_users, emb_dim]), name='user_embedding')\n        all_weights['item_embedding'] = tf.Variable(initializer([n_items, emb_dim]), name='item_embedding')\n        \n        weight_size_list = [emb_dim] + weight_size\n\n        for k in range(n_layers):\n            all_weights['W_gc_%d' %k] = tf.Variable(\n                initializer([weight_size_list[k], weight_size_list[k+1]]), name='W_gc_%d' % k)\n            all_weights['b_gc_%d' %k] = tf.Variable(\n                initializer([1, weight_size_list[k+1]]), name='b_gc_%d' % k)\n\n            all_weights['W_bi_%d' % k] = tf.Variable(\n                initializer([weight_size_list[k], weight_size_list[k + 1]]), name='W_bi_%d' % k)\n            all_weights['b_bi_%d' % k] = tf.Variable(\n                initializer([1, weight_size_list[k + 1]]), name='b_bi_%d' % k)\n\n            all_weights['W_mlp_%d' % k] = tf.Variable(\n                initializer([weight_size_list[k], weight_size_list[k+1]]), name='W_mlp_%d' % k)\n            all_weights['b_mlp_%d' % k] = tf.Variable(\n                initializer([1, weight_size_list[k+1]]), name='b_mlp_%d' % k)\n\n        return all_weights","metadata":{"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"weights = _init_weights( emb_dim = emb_dim, weight_size = weight_size, n_layers = n_layers)","metadata":{"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"## Accès aux données","metadata":{}},{"cell_type":"code","source":"# users, pos_items, neg_items = data_generator.sample()\n\n# un neg_item c'est un item du train qui n'est pas dans le batch\n\ndata_generator = Data(path='../Data/ml-100k', batch_size=batch_size)\n\n","metadata":{"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"n_users=943, n_items=1682\nn_interactions=100000\nn_train=90404, n_test=9596, sparsity=0.06305\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Pour générer un batch on exécute la méthode sample :","metadata":{}},{"cell_type":"code","source":"users, pos_items, neg_items = data_generator.sample()","metadata":{"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"#pos_items, users","metadata":{"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"On remarque que,marginalement, des items sont à la fois perçus comme positifs et négatifs","metadata":{}},{"cell_type":"code","source":"len(list(set(pos_items) & set(neg_items)))","metadata":{"trusted":true},"execution_count":30,"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"2"},"metadata":{}}]},{"cell_type":"markdown","source":"## Aperçu de la loss","metadata":{}},{"cell_type":"code","source":"tf.multiply(users, pos_items)","metadata":{"trusted":true},"execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"<tf.Tensor: shape=(64,), dtype=int32, numpy=\narray([ 228408,  211342,   51030,  122264,  497595,   13175,  667818,\n         96480,  382652,   83277,   63648,   77464,  273861,  156420,\n         26145,  109612,    4662,  292020,  261999,   48396,    6555,\n         10569,   14010,  253764,  278425,  201474,  188856,    9276,\n       1029129,   32175,    9106,   69480,  225388,  139896,   27900,\n        140996,  291797,  236080,  189270,  830308,  354090,  114448,\n          3856,  142690,   38448,    7808,  138047,  388080,   24444,\n        245841,  230272,  737019,   88198,  437570,  823528,   11700,\n        136290,  178437,    7068,  162165,  142330,  162434,   18270,\n        264421], dtype=int32)>"},"metadata":{}}]},{"cell_type":"markdown","source":"On constate qu'avec un argument négatif très fort la log sigmoid retourne - $\\infty$  : ","metadata":{}},{"cell_type":"code","source":"def create_bpr_loss(decay, batch_size, users, pos_items, neg_items):\n        pos_scores = tf.reduce_sum(tf.multiply(users, pos_items))\n        neg_scores = tf.reduce_sum(tf.multiply(users, neg_items))\n\n        regularizer = tf.nn.l2_loss(users) + tf.nn.l2_loss(pos_items) + tf.nn.l2_loss(neg_items)\n        regularizer = regularizer/batch_size\n        \n        # In the first version, we implement the bpr loss via the following codes:\n        # We report the performance in our paper using this implementation.\n        maxi = tf.math.log(tf.nn.sigmoid(pos_scores - neg_scores))\n        print(\"Maxi : \",maxi)\n        mf_loss = tf.negative(tf.reduce_mean(maxi))\n        print(\"MF loss V1 :\",mf_loss)\n        ## In the second version, we implement the bpr loss via the following codes to avoid 'NAN' loss during training:\n        ## However, it will change the training performance and training performance.\n        ## Please retrain the model and do a grid search for the best experimental setting.\n        mf_loss = tf.reduce_sum(tf.nn.softplus(-(pos_scores - neg_scores)))\n        print(\"MF loss V2 :\",mf_loss)\n\n        emb_loss = decay * regularizer\n\n        reg_loss = tf.constant(0.0, tf.float32, [1])\n\n        return pos_scores, neg_scores,  mf_loss, emb_loss, reg_loss","metadata":{"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"users2     = tf.constant(users, dtype='float32')\npos_items2 = tf.constant(pos_items, dtype='float32')\nneg_items2 = tf.constant(neg_items, dtype='float32')\n\ntf.multiply(users2, pos_items2)\npos_scores = tf.reduce_sum(tf.multiply(users2, pos_items2))\nneg_scores = tf.reduce_sum(tf.multiply(users2, neg_items2))\nprint(pos_scores - neg_scores)\n\n","metadata":{"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"tf.Tensor(-12241492.0, shape=(), dtype=float32)\n","output_type":"stream"}]},{"cell_type":"code","source":"create_bpr_loss(decay=decay, batch_size=batch_size, users=users2, pos_items=pos_items2, neg_items=neg_items2)","metadata":{"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"Maxi :  tf.Tensor(-inf, shape=(), dtype=float32)\nMF loss V1 : tf.Tensor(inf, shape=(), dtype=float32)\nMF loss V2 : tf.Tensor(12241492.0, shape=(), dtype=float32)\n","output_type":"stream"},{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"(<tf.Tensor: shape=(), dtype=float32, numpy=12710176.0>,\n <tf.Tensor: shape=(), dtype=float32, numpy=24951668.0>,\n <tf.Tensor: shape=(), dtype=float32, numpy=12241492.0>,\n <tf.Tensor: shape=(), dtype=float32, numpy=75838.6>,\n <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>)"},"metadata":{}}]},{"cell_type":"markdown","source":"L'ordre de grandeur de la loss v2 me parait très (trop) important et je garde en tête d'appliquer éventuellement un **clipping** car on peut avoir des risques d'explosion de gradient.","metadata":{}},{"cell_type":"markdown","source":"## Matrice d'adjacence","metadata":{}},{"cell_type":"markdown","source":"L'objet Data dans le module utiity/load_data contient l'outillage pour générer des matrices d'adjacence et les sauvegardée sur disque. Si les matrices ne sont pas déjà existentes elles sont crées alors dans le dossier Data du jeu de données en cours.","metadata":{}},{"cell_type":"code","source":"plain_adj, norm_adj, mean_adj = data_generator.get_adj_mat()","metadata":{"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"already load adj matrix (2625, 2625) 0.02720165252685547\n","output_type":"stream"}]},{"cell_type":"markdown","source":"3 matrices sont ainsi crées, on n'en utilise qu'une seule selon l'option précisée adj_type. J'ai une préférence pour une matrice normalisée.  ","metadata":{}},{"cell_type":"code","source":"norm_adj","metadata":{"trusted":true},"execution_count":36,"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"<2625x2625 sparse matrix of type '<class 'numpy.float64'>'\n\twith 183433 stored elements in Compressed Sparse Row format>"},"metadata":{}}]},{"cell_type":"code","source":"norm_adj[0,0],  norm_adj[25,30] ","metadata":{"trusted":true},"execution_count":37,"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"(0.0040650406504065045, 0.0)"},"metadata":{}}]},{"cell_type":"markdown","source":"Pour la suite on a besoin de l'attribut n_nonzero_elems :\n","metadata":{}},{"cell_type":"code","source":"n_nonzero_elems = norm_adj.count_nonzero()\nn_nonzero_elems","metadata":{"trusted":true},"execution_count":46,"outputs":[{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"183433"},"metadata":{}}]},{"cell_type":"markdown","source":"# Création des embeddings","metadata":{}},{"cell_type":"markdown","source":"Le code gère la problématique de saturation RAM en splittant la matrice d'adjacence dans une liste de sous-matrice et en prenant en compte possiblement l'utilisation de drop out.\n\nOn a besoin de pas mal d'outillage pour créer les embeddings :","metadata":{}},{"cell_type":"code","source":"def _create_ngcf_embed(node_dropout_flag , norm_adj, weights, mess_dropout, n_users, n_items, n_layers, n_fold = n_fold, node_dropout = node_dropout):\n# Generate a set of adjacency sub-matrix.\n    if node_dropout_flag:\n        # node dropout.\n        A_fold_hat = _split_A_hat_node_dropout(norm_adj)\n    else:\n        A_fold_hat = _split_A_hat(norm_adj)\n\n    ego_embeddings = tf.concat([weights['user_embedding'], weights['item_embedding']], axis=0)\n\n    all_embeddings = [ego_embeddings]\n\n    for k in range(0, n_layers):\n\n        temp_embed = []\n        for f in range(n_fold):\n            temp_embed.append(tf.sparse.sparse_dense_matmul(A_fold_hat[f], ego_embeddings))\n\n        # sum messages of neighbors.\n        side_embeddings = tf.concat(temp_embed, 0)\n        # transformed sum messages of neighbors.\n        sum_embeddings = tf.nn.leaky_relu(\n            tf.matmul(side_embeddings, weights['W_gc_%d' % k]) + weights['b_gc_%d' % k])\n\n        # bi messages of neighbors.\n        bi_embeddings = tf.multiply(ego_embeddings, side_embeddings)\n        # transformed bi messages of neighbors.\n        bi_embeddings = tf.nn.leaky_relu(\n            tf.matmul(bi_embeddings, weights['W_bi_%d' % k]) + weights['b_bi_%d' % k])\n\n        # non-linear activation.\n        ego_embeddings = sum_embeddings + bi_embeddings\n\n        # message dropout.\n        ego_embeddings = tf.nn.dropout(ego_embeddings, 1 - mess_dropout[k])\n\n        # normalize the distribution of embeddings.\n        norm_embeddings = tf.nn.l2_normalize(ego_embeddings, axis=1)\n\n        all_embeddings += [norm_embeddings]\n\n    all_embeddings = tf.concat(all_embeddings, 1)\n    u_g_embeddings, i_g_embeddings = tf.split(all_embeddings, [n_users, n_items], 0)\n    return u_g_embeddings, i_g_embeddings\n\n\n\ndef _convert_sp_mat_to_sp_tensor(X):\n    coo = X.tocoo().astype(np.float32)\n    indices = np.mat([coo.row, coo.col]).transpose()\n    return tf.SparseTensor(indices, coo.data, coo.shape)\n\ndef _dropout_sparse(X, keep_prob, n_nonzero_elems):\n    \"\"\"\n    Dropout for sparse tensors.\n    \"\"\"\n    noise_shape = [n_nonzero_elems]\n    random_tensor = keep_prob\n    random_tensor += tf.random.uniform(noise_shape)\n    dropout_mask = tf.cast(tf.floor(random_tensor), dtype=tf.bool)\n    pre_out = tf.sparse.retain(X, dropout_mask)\n\n    return pre_out * tf.math.divide(1., keep_prob)\n\n\n\ndef _split_A_hat(X):\n    A_fold_hat = []\n\n    fold_len = (n_users + n_items) // n_fold\n    for i_fold in range(n_fold):\n        start = i_fold * fold_len\n        if i_fold == n_fold -1:\n            end = n_users + n_items\n        else:\n            end = (i_fold + 1) * fold_len\n\n        A_fold_hat.append(_convert_sp_mat_to_sp_tensor(X[start:end]))\n    return A_fold_hat\n\ndef _split_A_hat_node_dropout( X):\n    A_fold_hat = []\n\n    fold_len = (n_users + n_items) // n_fold\n    for i_fold in range(n_fold):\n        start = i_fold * fold_len\n        if i_fold == n_fold -1:\n            end = n_users + n_items\n        else:\n            end = (i_fold + 1) * fold_len\n\n        # A_fold_hat.append(self._convert_sp_mat_to_sp_tensor(X[start:end]))\n        temp = _convert_sp_mat_to_sp_tensor(X[start:end])\n        n_nonzero_temp = X[start:end].count_nonzero()\n        A_fold_hat.append(_dropout_sparse(temp, 1 - node_dropout[0], n_nonzero_temp))\n\n    return A_fold_hat\n\n","metadata":{"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"_create_ngcf_embed(node_dropout_flag=1 , norm_adj = norm_adj , weights = weights , node_dropout = node_dropout, mess_dropout = mess_dropout, n_users = n_users , n_items = n_items,n_layers = n_layers, n_fold = 100)","metadata":{"trusted":true},"execution_count":64,"outputs":[{"execution_count":64,"output_type":"execute_result","data":{"text/plain":"(<tf.Tensor: shape=(943, 168), dtype=float32, numpy=\n array([[ 0.01808818, -0.0551286 ,  0.04615904, ...,  0.        ,\n          0.        ,  0.        ],\n        [-0.07797763,  0.03643166,  0.02175254, ...,  0.        ,\n          0.        ,  0.        ],\n        [-0.00193043, -0.07539124,  0.06724135, ...,  0.        ,\n          0.28094506, -0.        ],\n        ...,\n        [ 0.04744322,  0.05934034,  0.05376158, ...,  0.        ,\n         -0.        , -0.        ],\n        [-0.04016246, -0.00218273, -0.04625681, ...,  0.        ,\n          0.        ,  0.        ],\n        [-0.00753046, -0.06106514, -0.07502777, ...,  0.        ,\n          0.        ,  0.        ]], dtype=float32)>,\n <tf.Tensor: shape=(1682, 168), dtype=float32, numpy=\n array([[-0.00897326,  0.03025502,  0.00671209, ...,  0.        ,\n          0.        , -0.        ],\n        [ 0.00470646, -0.03025699,  0.04256403, ...,  0.        ,\n          0.        ,  0.        ],\n        [-0.02412724,  0.01497417,  0.04873864, ...,  0.        ,\n          0.        , -0.048783  ],\n        ...,\n        [ 0.01617029,  0.05204433,  0.03066313, ..., -0.        ,\n          0.        , -0.        ],\n        [-0.04818759,  0.00682431,  0.01187072, ..., -0.        ,\n          0.        , -0.        ],\n        [ 0.01880764,  0.0456925 , -0.00457004, ...,  0.6703308 ,\n          0.        , -0.        ]], dtype=float32)>)"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class NGCF(object):\n    def __init__(self, n_users, n_items, emb_dim, weight_size, n_layers):\n        self.n_users = n_users\n        self.n_items = n_items\n        self.emb_dim = emb_dim\n        self.weight_size = weight_size\n        self.weight_size_list = 0\n        self.n_layers = n_layers\n        self.weights = self._init_weights()\n        \n        \n        self.ua_embeddings, self.ia_embeddings = self._create_ngcf_embed()\n        \n        self.u_g_embeddings = tf.nn.embedding_lookup(self.ua_embeddings, self.users)\n        self.pos_i_g_embeddings = tf.nn.embedding_lookup(self.ia_embeddings, self.pos_items)\n        self.neg_i_g_embeddings = tf.nn.embedding_lookup(self.ia_embeddings, self.neg_items)\n        \n        \n        ego_embeddings = tf.concat([self.weights['user_embedding'], self.weights['item_embedding']], axis=0)\n        all_embeddings = [ego_embeddings]\n         # Original embedding.\n        u_e = tf.nn.embedding_lookup(self.weights['user_embedding'], self.users)\n        pos_i_e = tf.nn.embedding_lookup(self.weights['item_embedding'], self.pos_items)\n        neg_i_e = tf.nn.embedding_lookup(self.weights['item_embedding'], self.neg_items)\n\n        # All ratings for all users.\n        #self.batch_ratings = self._create_batch_ratings(u_e, pos_i_e)\n        \n        self.mf_loss, self.emb_loss, self.reg_loss = self.create_bpr_loss(u_e, pos_i_e, neg_i_e)\n        self.loss = self.mf_loss + self.emb_loss + self.reg_loss\n\n        # self.dy_lr = tf.train.exponential_decay(self.lr, self.global_step, 10000, self.lr_decay, staircase=True)\n        # self.opt = tf.train.RMSPropOptimizer(learning_rate=self.dy_lr).minimize(self.loss, global_step=self.global_step)\n        self.opt = tf.train.RMSPropOptimizer(learning_rate=self.lr).minimize(self.loss)\n        # self.updates = self.opt.minimize(self.loss, var_list=self.weights)\n\n        \n        \n    def _init_weights(self):\n        all_weights = dict()\n\n        initializer = tf.keras.initializers.GlorotUniform()\n        all_weights['user_embedding'] = tf.Variable(initializer([self.n_users, self.emb_dim]), name='user_embedding')\n        all_weights['item_embedding'] = tf.Variable(initializer([self.n_items, self.emb_dim]), name='item_embedding')\n        \n        self.weight_size_list = [self.emb_dim] + self.weight_size\n\n        for k in range(self.n_layers):\n            all_weights['W_gc_%d' %k] = tf.Variable(\n                initializer([self.weight_size_list[k], self.weight_size_list[k+1]]), name='W_gc_%d' % k)\n            all_weights['b_gc_%d' %k] = tf.Variable(\n                initializer([1, self.weight_size_list[k+1]]), name='b_gc_%d' % k)\n\n            all_weights['W_bi_%d' % k] = tf.Variable(\n                initializer([self.weight_size_list[k], self.weight_size_list[k + 1]]), name='W_bi_%d' % k)\n            all_weights['b_bi_%d' % k] = tf.Variable(\n                initializer([1, self.weight_size_list[k + 1]]), name='b_bi_%d' % k)\n\n            all_weights['W_mlp_%d' % k] = tf.Variable(\n                initializer([self.weight_size_list[k], self.weight_size_list[k+1]]), name='W_mlp_%d' % k)\n            all_weights['b_mlp_%d' % k] = tf.Variable(\n                initializer([1, self.weight_size_list[k+1]]), name='b_mlp_%d' % k)\n\n        return all_weights\n    \n    def create_bpr_loss(decay, batch_size, users, pos_items, neg_items):\n        pos_scores = tf.reduce_sum(tf.multiply(users, pos_items))\n        neg_scores = tf.reduce_sum(tf.multiply(users, neg_items))\n\n        regularizer = tf.nn.l2_loss(users) + tf.nn.l2_loss(pos_items) + tf.nn.l2_loss(neg_items)\n        regularizer = regularizer/batch_size\n        \n        # In the first version, we implement the bpr loss via the following codes:\n        # We report the performance in our paper using this implementation.\n        #maxi = tf.math.log(tf.nn.sigmoid(pos_scores - neg_scores))\n        #mf_loss = tf.negative(tf.reduce_mean(maxi))\n        \n        ## In the second version, we implement the bpr loss via the following codes to avoid 'NAN' loss during training:\n        ## However, it will change the training performance and training performance.\n        ## Please retrain the model and do a grid search for the best experimental setting.\n        mf_loss = tf.reduce_sum(tf.nn.softplus(-(pos_scores - neg_scores)))\n        \n\n        emb_loss = decay * regularizer\n\n        reg_loss = tf.constant(0.0, tf.float32, [1])\n\n        return pos_scores, neg_scores,  mf_loss, emb_loss, reg_loss\n    \n    def _create_ngcf_embed(self):\n        # Generate a set of adjacency sub-matrix.\n        if self.node_dropout_flag:\n            # node dropout.\n            A_fold_hat = self._split_A_hat_node_dropout(self.norm_adj)\n        else:\n            A_fold_hat = self._split_A_hat(self.norm_adj)\n\n        ego_embeddings = tf.concat([self.weights['user_embedding'], self.weights['item_embedding']], axis=0)\n\n        all_embeddings = [ego_embeddings]\n\n        for k in range(0, self.n_layers):\n\n            temp_embed = []\n            for f in range(self.n_fold):\n                temp_embed.append(tf.sparse_tensor_dense_matmul(A_fold_hat[f], ego_embeddings))\n\n            # sum messages of neighbors.\n            side_embeddings = tf.concat(temp_embed, 0)\n            # transformed sum messages of neighbors.\n            sum_embeddings = tf.nn.leaky_relu(\n                tf.matmul(side_embeddings, self.weights['W_gc_%d' % k]) + self.weights['b_gc_%d' % k])\n\n            # bi messages of neighbors.\n            bi_embeddings = tf.multiply(ego_embeddings, side_embeddings)\n            # transformed bi messages of neighbors.\n            bi_embeddings = tf.nn.leaky_relu(\n                tf.matmul(bi_embeddings, self.weights['W_bi_%d' % k]) + self.weights['b_bi_%d' % k])\n\n            # non-linear activation.\n            ego_embeddings = sum_embeddings + bi_embeddings\n\n            # message dropout.\n            ego_embeddings = tf.nn.dropout(ego_embeddings, 1 - self.mess_dropout[k])\n\n            # normalize the distribution of embeddings.\n            norm_embeddings = tf.math.l2_normalize(ego_embeddings, axis=1)\n\n            all_embeddings += [norm_embeddings]\n\n        all_embeddings = tf.concat(all_embeddings, 1)\n        u_g_embeddings, i_g_embeddings = tf.split(all_embeddings, [self.n_users, self.n_items], 0)\n        return u_g_embeddings, i_g_embeddings\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = NGCF(n_users, n_items, emb_dim, weight_size,n_layers)\npoids = model._init_weights()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"poids","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"initializer = tf.keras.initializers.GlorotUniform()\n\nmodel = keras.Sequential(\n    [\n        tf.keras.layers.Embedding(input_dim, output_dim,embeddings_initializer='GlorotUniform'),\n        layers.Dense(3, activation=\"relu\", name=\"layer2\"),\n        layers.Dense(4, name=\"layer3\"),\n    ]\n)\n# Call model on a test input\nx = tf.ones((3, 3))\ny = model(x)\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = tf.Variable(tf.random.uniform([5, 30], -1, 1))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"split0, split1, split2 = tf.split(x, [4, 15, 11], 1)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"split0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}